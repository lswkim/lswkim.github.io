---
title: "[FastAPI ì‹¤ì „ì‹¬í™” 6] Python ê°ì²´/ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ íŒ¨í„´"
date: 2025-12-29 15:00:00 +0900
categories: [Tech, FastAPI]
tags: [python, fastapi, resource-management, context-manager, lifespan]
mermaid: true
---

> **ğŸ“š FastAPI ì‹œë¦¬ì¦ˆ - Part 5. ì‹¤ì „ ì‹¬í™”**
>
> 1. [ë™ê¸° í•¨ìˆ˜ vs ë¹„ë™ê¸° í•¨ìˆ˜ ì„ íƒ ê¸°ì¤€](/posts/sync-async-choice/)
> 2. [BackgroundTasksì™€ ì‘ì—… í](/posts/background-tasks/)
> 3. [ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ì™€ ì„±ëŠ¥ íŠœë‹](/posts/concurrency-tuning/)
> 4. [FastAPI ì˜ˆì™¸ì²˜ë¦¬](/posts/exception-handling/)
> 5. [í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ê³„](/posts/project-structure/)
> 6. Python ê°ì²´/ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ íŒ¨í„´ â† í˜„ì¬ ê¸€

---

# 6. Python ê°ì²´/ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ íŒ¨í„´

## ì™œ ê°ì²´/ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ê°€ ì¤‘ìš”í•œê°€?

- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- ì„±ëŠ¥ ìµœì í™”
- ë™ì‹œì„± ì œì–´
- ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜ ë°©ì§€

---

## íŒ¨í„´ ë¶„ë¥˜

```mermaid
flowchart TB
    subgraph Creation["ìƒì„± íŒ¨í„´ - ëª‡ ê°œ ë§Œë“¤ ê²ƒì¸ê°€?"]
        C1[Singleton â†’ ë”± 1ê°œë§Œ]
        C2[Factory â†’ ìƒì„± ë¡œì§ ë¶„ë¦¬]
        C3[Pool â†’ ë¯¸ë¦¬ Nê°œ ë§Œë“¤ì–´ë‘ê³  ì¬ì‚¬ìš©]
        C4[Lazy Loading â†’ í•„ìš”í•  ë•Œ ìƒì„±]
    end

    subgraph Concurrency["ë™ì‹œì„± ì œì–´ - ë™ì‹œì— ëª‡ ê°œê¹Œì§€ ì ‘ê·¼?"]
        CO1[Semaphore â†’ ë™ì‹œ Nê°œê¹Œì§€ í—ˆìš©]
        CO2[Lock/Mutex â†’ ë™ì‹œ 1ê°œë§Œ í—ˆìš©]
        CO3[Rate Limiter â†’ ì‹œê°„ë‹¹ Nê°œ í—ˆìš©]
    end

    subgraph Lifecycle["ìˆ˜ëª… ê´€ë¦¬ - ì–¸ì œ ìƒì„±/í•´ì œ?"]
        L1[Context Manager â†’ withë¬¸ìœ¼ë¡œ ìë™ ì •ë¦¬]
        L2[Dependency Injection â†’ í”„ë ˆì„ì›Œí¬ê°€ ê´€ë¦¬]
    end
```

---

## Part 1: ìƒì„± íŒ¨í„´

### 1. Singleton (ì‹±ê¸€í†¤)

### ê°œë…

```
ì¸ìŠ¤í„´ìŠ¤ê°€ ì˜¤ì§ 1ê°œë§Œ ì¡´ì¬í•˜ë„ë¡ ë³´ì¥
```

### ì–¸ì œ ì‚¬ìš©?

| ì‚¬ìš© O | ì‚¬ìš© X |
| --- | --- |
| ML ëª¨ë¸ | DB ì„¸ì…˜ (ìš”ì²­ë³„ í•„ìš”) |
| ì„¤ì • ê°ì²´ | ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ |
| LLM í´ë¼ì´ì–¸íŠ¸ | íŠ¸ëœì­ì…˜ |
| HTTP í´ë¼ì´ì–¸íŠ¸ | ìš”ì²­ë³„ ë°ì´í„° |

### êµ¬í˜„

```python
from typing import Optional

class ModelManager:
    """ì‹±ê¸€í†¤ - ëª¨ë¸ ë§¤ë‹ˆì €"""

    _instance: Optional["ModelManager"] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return

        self.model = None
        self._initialized = True

    def load(self, path: str):
        self.model = load_model(path)

    def predict(self, data):
        return self.model.predict(data)

# ì‚¬ìš© - ì–´ë””ì„œë“  ê°™ì€ ì¸ìŠ¤í„´ìŠ¤
manager1 = ModelManager()
manager2 = ModelManager()
print(manager1 is manager2)  # True

```

### FastAPI ì ìš©

```python
# lifespanì—ì„œ ì´ˆê¸°í™”
@asynccontextmanager
async def lifespan(app: FastAPI):
    model_manager = ModelManager()
    await model_manager.load("models/")
    app.state.model_manager = model_manager
    yield
    await model_manager.cleanup()

# ì˜ì¡´ì„±ìœ¼ë¡œ ì£¼ì…
def get_model_manager(request: Request) -> ModelManager:
    return request.app.state.model_manager

@router.post("/predict")
async def predict(
    data: Input,
    model: ModelManager = Depends(get_model_manager)
):
    return model.predict(data)

```

---

### 2. Factory (íŒ©í† ë¦¬)

### ê°œë…

```
ê°ì²´ ìƒì„± ë¡œì§ì„ ë³„ë„ í´ë˜ìŠ¤/í•¨ìˆ˜ë¡œ ë¶„ë¦¬
â†’ ìƒì„± ë¡œì§ ë³€ê²½ ì‹œ í•œ ê³³ë§Œ ìˆ˜ì •
â†’ ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ê°ì²´ ìƒì„±
```

### ì–¸ì œ ì‚¬ìš©?

| ì‚¬ìš© O | ì‚¬ìš© X |
| --- | --- |
| LLM ì–´ëŒ‘í„° (OpenAI/Anthropic) | ë‹¨ìˆœ ê°ì²´ |
| VectorStore ì–´ëŒ‘í„° | ìƒì„± ë¡œì§ì´ ë‹¨ìˆœí•  ë•Œ |
| í™˜ê²½ë³„ ë‹¤ë¥¸ ì„¤ì • |  |

### êµ¬í˜„

```python
from abc import ABC, abstractmethod

# ì¶”ìƒ ì¸í„°í˜ì´ìŠ¤
class LLMClient(ABC):
    @abstractmethod
    async def chat(self, messages: list) -> str:
        pass

# êµ¬í˜„ì²´ë“¤
class OpenAIClient(LLMClient):
    def __init__(self, api_key: str):
        self.client = AsyncOpenAI(api_key=api_key)

    async def chat(self, messages: list) -> str:
        response = await self.client.chat.completions.create(
            model="gpt-4",
            messages=messages
        )
        return response.choices[0].message.content

class AnthropicClient(LLMClient):
    def __init__(self, api_key: str):
        self.client = AsyncAnthropic(api_key=api_key)

    async def chat(self, messages: list) -> str:
        response = await self.client.messages.create(
            model="claude-3-sonnet",
            messages=messages,
            max_tokens=4096
        )
        return response.content[0].text

class BedrockClient(LLMClient):
    def __init__(self, region: str):
        self.client = boto3.client("bedrock-runtime", region_name=region)

    async def chat(self, messages: list) -> str:
        # Bedrock êµ¬í˜„
        pass

# íŒ©í† ë¦¬
class LLMClientFactory:
    """LLM í´ë¼ì´ì–¸íŠ¸ íŒ©í† ë¦¬"""

    @staticmethod
    def create(provider: str, **kwargs) -> LLMClient:
        if provider == "openai":
            return OpenAIClient(api_key=kwargs["api_key"])
        elif provider == "anthropic":
            return AnthropicClient(api_key=kwargs["api_key"])
        elif provider == "bedrock":
            return BedrockClient(region=kwargs["region"])
        else:
            raise ValueError(f"Unknown provider: {provider}")

# ì‚¬ìš©
client = LLMClientFactory.create(
    provider=settings.LLM_PROVIDER,
    api_key=settings.LLM_API_KEY
)

```

### ì„¤ì • ê¸°ë°˜ íŒ©í† ë¦¬

```python
# config.py
class Settings(BaseSettings):
    LLM_PROVIDER: str = "openai"  # openai, anthropic, bedrock
    LLM_API_KEY: str = ""
    LLM_REGION: str = "us-east-1"

# factory.py
def create_llm_client(settings: Settings) -> LLMClient:
    """ì„¤ì •ì— ë”°ë¼ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""

    config = {
        "openai": lambda: OpenAIClient(settings.LLM_API_KEY),
        "anthropic": lambda: AnthropicClient(settings.LLM_API_KEY),
        "bedrock": lambda: BedrockClient(settings.LLM_REGION),
    }

    factory = config.get(settings.LLM_PROVIDER)
    if not factory:
        raise ValueError(f"Unknown provider: {settings.LLM_PROVIDER}")

    return factory()

```

---

### 3. Pool (í’€)

### ê°œë…

```
ë¯¸ë¦¬ Nê°œì˜ ë¦¬ì†ŒìŠ¤ë¥¼ ë§Œë“¤ì–´ë‘ê³  ì¬ì‚¬ìš©
â†’ ìƒì„± ë¹„ìš©ì´ ë¹„ì‹¼ ë¦¬ì†ŒìŠ¤ì— ì í•©
â†’ ì‚¬ìš© í›„ ë°˜í™˜, ë‹¤ë¥¸ ìš”ì²­ì´ ì¬ì‚¬ìš©
```

### ì–¸ì œ ì‚¬ìš©?

| ì‚¬ìš© O | ì‚¬ìš© X |
| --- | --- |
| DB ì»¤ë„¥ì…˜ | ê°€ë²¼ìš´ ê°ì²´ |
| HTTP ì»¤ë„¥ì…˜ | ìƒì„± ë¹„ìš©ì´ ì €ë ´í•œ ê²ƒ |
| ìŠ¤ë ˆë“œ í’€ | ìƒíƒœê°€ ìˆëŠ” ê°ì²´ |

### DB ì»¤ë„¥ì…˜ í’€

```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession

# ë™ê¸° ì»¤ë„¥ì…˜ í’€
engine = create_engine(
    DATABASE_URL,
    pool_size=10,          # ê¸°ë³¸ ì»¤ë„¥ì…˜ ìˆ˜
    max_overflow=20,       # ì¶”ê°€ í—ˆìš© ì»¤ë„¥ì…˜
    pool_recycle=1800,     # 30ë¶„ë§ˆë‹¤ ì¬ìƒì„±
    pool_pre_ping=True,    # ì‚¬ìš© ì „ ì—°ê²° ì²´í¬
)

# ë¹„ë™ê¸° ì»¤ë„¥ì…˜ í’€
async_engine = create_async_engine(
    ASYNC_DATABASE_URL,
    pool_size=10,
    max_overflow=20,
)

# ì„¸ì…˜ íŒ©í† ë¦¬
SessionLocal = sessionmaker(bind=engine)
AsyncSessionLocal = sessionmaker(
    bind=async_engine,
    class_=AsyncSession,
    expire_on_commit=False
)

# FastAPI ì˜ì¡´ì„± - í’€ì—ì„œ ê°€ì ¸ì™€ ì‚¬ìš© í›„ ë°˜í™˜
async def get_db():
    async with AsyncSessionLocal() as session:
        yield session  # ì‚¬ìš© í›„ ìë™ ë°˜í™˜

```

### HTTP ì»¤ë„¥ì…˜ í’€

```python
import httpx

class HTTPClientPool:
    """HTTP í´ë¼ì´ì–¸íŠ¸ í’€ (ì‹±ê¸€í†¤)"""

    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return
        self.client = None
        self._initialized = True

    async def initialize(self):
        self.client = httpx.AsyncClient(
            limits=httpx.Limits(
                max_connections=100,      # ìµœëŒ€ ì»¤ë„¥ì…˜
                max_keepalive_connections=20,  # ìœ ì§€í•  ì»¤ë„¥ì…˜
            ),
            timeout=httpx.Timeout(30.0),
        )

    async def get(self, url: str, **kwargs):
        return await self.client.get(url, **kwargs)

    async def post(self, url: str, **kwargs):
        return await self.client.post(url, **kwargs)

    async def close(self):
        if self.client:
            await self.client.aclose()

```

### ì»¤ìŠ¤í…€ í’€ êµ¬í˜„

```python
import asyncio
from typing import Generic, TypeVar, Callable
from collections import deque

T = TypeVar("T")

class ObjectPool(Generic[T]):
    """ë²”ìš© ê°ì²´ í’€"""

    def __init__(
        self,
        factory: Callable[[], T],
        max_size: int = 10,
        min_size: int = 2
    ):
        self.factory = factory
        self.max_size = max_size
        self.min_size = min_size
        self._pool: deque[T] = deque()
        self._in_use: set[T] = set()
        self._lock = asyncio.Lock()

    async def initialize(self):
        """ìµœì†Œ ê°œìˆ˜ë§Œí¼ ë¯¸ë¦¬ ìƒì„±"""
        for _ in range(self.min_size):
            obj = self.factory()
            self._pool.append(obj)

    async def acquire(self) -> T:
        """í’€ì—ì„œ ê°ì²´ ê°€ì ¸ì˜¤ê¸°"""
        async with self._lock:
            if self._pool:
                obj = self._pool.popleft()
            elif len(self._in_use) < self.max_size:
                obj = self.factory()
            else:
                raise RuntimeError("Pool exhausted")

            self._in_use.add(obj)
            return obj

    async def release(self, obj: T):
        """ê°ì²´ ë°˜í™˜"""
        async with self._lock:
            self._in_use.discard(obj)
            if len(self._pool) < self.max_size:
                self._pool.append(obj)

# ì‚¬ìš© ì˜ˆì‹œ
gpu_pool = ObjectPool(
    factory=lambda: GPUWorker(),
    max_size=4,
    min_size=2
)

async def process():
    worker = await gpu_pool.acquire()
    try:
        result = await worker.process(data)
        return result
    finally:
        await gpu_pool.release(worker)

```

---

### 4. Lazy Loading (ì§€ì—° ë¡œë”©)

### ê°œë…

```
í•„ìš”í•  ë•Œ ìƒì„± (ì²˜ìŒ ì ‘ê·¼ ì‹œ)
â†’ ì‹œì‘ ì‹œê°„ ë‹¨ì¶•
â†’ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„± ì•ˆ í•¨
```

### ì–¸ì œ ì‚¬ìš©?

| ì‚¬ìš© O | ì‚¬ìš© X |
| --- | --- |
| ì„ íƒì  ê¸°ëŠ¥ì˜ ëª¨ë¸ | í•­ìƒ í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ |
| ë¬´ê±°ìš´ ì´ˆê¸°í™” | ë¹ ë¥¸ ì‘ë‹µ í•„ìš”í•  ë•Œ |
| ì¡°ê±´ë¶€ ì‚¬ìš© ë¦¬ì†ŒìŠ¤ |  |

### êµ¬í˜„

```python
class LazyModelLoader:
    """ì§€ì—° ë¡œë”© ëª¨ë¸ ë§¤ë‹ˆì €"""

    def __init__(self):
        self._models: dict = {}
        self._model_paths: dict = {
            "classifier": "models/classifier.onnx",
            "detector": "models/detector.onnx",
            "segmenter": "models/segmenter.onnx",
        }

    def get_model(self, name: str):
        """í•„ìš”í•  ë•Œ ë¡œë”©"""
        if name not in self._models:
            path = self._model_paths.get(name)
            if not path:
                raise ValueError(f"Unknown model: {name}")

            print(f"Loading model: {name}")  # ì²« ì ‘ê·¼ ì‹œë§Œ ë¡œë”©
            self._models[name] = load_model(path)

        return self._models[name]

# ì‚¬ìš©
loader = LazyModelLoader()

# classifierê°€ í•„ìš”í•  ë•Œë§Œ ë¡œë”©
result1 = loader.get_model("classifier").predict(data)

# detectorëŠ” í˜¸ì¶œ ì•ˆ í•˜ë©´ ë¡œë”© ì•ˆ ë¨

```

### Propertyë¥¼ í™œìš©í•œ Lazy Loading

```python
class ServiceContainer:
    """ì„œë¹„ìŠ¤ ì»¨í…Œì´ë„ˆ - Lazy Loading"""

    def __init__(self, settings):
        self.settings = settings
        self._llm_client = None
        self._vector_store = None
        self._db_session = None

    @property
    def llm_client(self):
        """LLM í´ë¼ì´ì–¸íŠ¸ - ì²« ì ‘ê·¼ ì‹œ ìƒì„±"""
        if self._llm_client is None:
            self._llm_client = LLMClientFactory.create(
                provider=self.settings.LLM_PROVIDER,
                api_key=self.settings.LLM_API_KEY
            )
        return self._llm_client

    @property
    def vector_store(self):
        """VectorStore - ì²« ì ‘ê·¼ ì‹œ ìƒì„±"""
        if self._vector_store is None:
            self._vector_store = VectorStoreFactory.create(
                provider=self.settings.VECTOR_STORE_PROVIDER
            )
        return self._vector_store

# ì‚¬ìš©
container = ServiceContainer(settings)

# llm_client ì ‘ê·¼ ì‹œì—ë§Œ ìƒì„±
response = await container.llm_client.chat(messages)

```

---

## Part 2: ë™ì‹œì„± ì œì–´ íŒ¨í„´

### 1. Semaphore (ì„¸ë§ˆí¬ì–´)

### ê°œë…

```
ë™ì‹œì— Nê°œê¹Œì§€ë§Œ ì ‘ê·¼ í—ˆìš©
â†’ ë¦¬ì†ŒìŠ¤ ê³¼ë¶€í•˜ ë°©ì§€
â†’ N=1ì´ë©´ Lockê³¼ ë™ì¼
```

### ì–¸ì œ ì‚¬ìš©?

| ì‚¬ìš© O | ì‚¬ìš© X |
| --- | --- |
| GPU ë™ì‹œ ì‚¬ìš© ì œí•œ | ìˆœì„œê°€ ì¤‘ìš”í•œ ì‘ì—… |
| ì™¸ë¶€ API ë™ì‹œ í˜¸ì¶œ ì œí•œ | ë‹¨ì¼ ì ‘ê·¼ë§Œ í•„ìš”í•  ë•Œ |
| DB ì»¤ë„¥ì…˜ ì œí•œ |  |

### êµ¬í˜„

```python
import asyncio

class GPUInferenceService:
    """GPU ì¶”ë¡  ì„œë¹„ìŠ¤ - ë™ì‹œ ì‹¤í–‰ ì œí•œ"""

    def __init__(self, max_concurrent: int = 4):
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.model = None

    async def predict(self, data):
        """ìµœëŒ€ 4ê°œê¹Œì§€ë§Œ ë™ì‹œ ì‹¤í–‰"""
        async with self.semaphore:
            # ì´ ë¸”ë¡ ì•ˆì—ëŠ” ìµœëŒ€ 4ê°œ ìš”ì²­ë§Œ ë™ì‹œ ì§„ì…
            result = await self._do_inference(data)
            return result

    async def _do_inference(self, data):
        # GPU ì¶”ë¡  ì‹¤í–‰
        return self.model.predict(data)

# ì‚¬ìš©
service = GPUInferenceService(max_concurrent=4)

# 100ê°œ ìš”ì²­ì´ ì™€ë„ ë™ì‹œì— 4ê°œë§Œ ì‹¤í–‰
results = await asyncio.gather(*[
    service.predict(data) for data in data_list
])

```

### ì™¸ë¶€ API í˜¸ì¶œ ì œí•œ

```python
class ExternalAPIClient:
    """ì™¸ë¶€ API í´ë¼ì´ì–¸íŠ¸ - ë™ì‹œ í˜¸ì¶œ ì œí•œ"""

    def __init__(self, max_concurrent: int = 10):
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.client = httpx.AsyncClient()

    async def call_api(self, endpoint: str, data: dict):
        """ë™ì‹œ ìµœëŒ€ 10ê°œ í˜¸ì¶œ"""
        async with self.semaphore:
            response = await self.client.post(endpoint, json=data)
            return response.json()

# OpenAI API í˜¸ì¶œ ì œí•œ
class OpenAIRateLimitedClient:
    """OpenAI API - Rate Limit ëŒ€ì‘"""

    def __init__(self, api_key: str, max_concurrent: int = 50):
        self.client = AsyncOpenAI(api_key=api_key)
        self.semaphore = asyncio.Semaphore(max_concurrent)

    async def chat(self, messages: list):
        async with self.semaphore:
            return await self.client.chat.completions.create(
                model="gpt-4",
                messages=messages
            )

```

### FastAPI ì—”ë“œí¬ì¸íŠ¸ ë™ì‹œ ì‹¤í–‰ ì œí•œ

```python
# ê¸€ë¡œë²Œ ì„¸ë§ˆí¬ì–´
inference_semaphore = asyncio.Semaphore(10)

@router.post("/heavy-task")
async def heavy_task(data: Input):
    """ë¬´ê±°ìš´ ì‘ì—… - ë™ì‹œ 10ê°œ ì œí•œ"""
    async with inference_semaphore:
        result = await process_heavy_task(data)
        return result

# ë˜ëŠ” ì˜ì¡´ì„±ìœ¼ë¡œ
async def get_semaphore_slot():
    """ì„¸ë§ˆí¬ì–´ ìŠ¬ë¡¯ íšë“"""
    async with inference_semaphore:
        yield

@router.post("/heavy-task")
async def heavy_task(
    data: Input,
    _: None = Depends(get_semaphore_slot)
):
    return await process_heavy_task(data)

```

---

### 2. Lock / Mutex (ë½ / ë®¤í…ìŠ¤)

### ê°œë…

```
ë™ì‹œì— 1ê°œë§Œ ì ‘ê·¼ í—ˆìš©
â†’ ê³µìœ  ìì› ë³´í˜¸
â†’ ê²½ìŸ ìƒíƒœ(Race Condition) ë°©ì§€
```

### ì–¸ì œ ì‚¬ìš©?

| ì‚¬ìš© O | ì‚¬ìš© X |
| --- | --- |
| ê³µìœ  ìƒíƒœ ìˆ˜ì • | ì½ê¸° ì „ìš© |
| íŒŒì¼ ì“°ê¸° | ë…ë¦½ì ì¸ ì‘ì—… |
| ì¹´ìš´í„° ì¦ê°€ |  |

### êµ¬í˜„

```python
import asyncio
from threading import Lock

# ë¹„ë™ê¸° Lock
class Counter:
    """ìŠ¤ë ˆë“œ ì•ˆì „ ì¹´ìš´í„°"""

    def __init__(self):
        self.value = 0
        self.lock = asyncio.Lock()

    async def increment(self):
        async with self.lock:
            # ì´ ë¸”ë¡ì€ í•œ ë²ˆì— í•˜ë‚˜ë§Œ ì‹¤í–‰
            current = self.value
            await asyncio.sleep(0.001)  # ì‹œë®¬ë ˆì´ì…˜
            self.value = current + 1
            return self.value

# ë™ê¸° Lock (ë©€í‹°ìŠ¤ë ˆë”©)
class ThreadSafeCounter:
    def __init__(self):
        self.value = 0
        self.lock = Lock()

    def increment(self):
        with self.lock:
            self.value += 1
            return self.value

```

### íŒŒì¼ ì“°ê¸° ë³´í˜¸

```python
class LogWriter:
    """ë¡œê·¸ íŒŒì¼ ì“°ê¸° - ë™ì‹œ ì ‘ê·¼ ë°©ì§€"""

    def __init__(self, filepath: str):
        self.filepath = filepath
        self.lock = asyncio.Lock()

    async def write(self, message: str):
        async with self.lock:
            async with aiofiles.open(self.filepath, "a") as f:
                await f.write(f"{datetime.now()}: {message}\n")

```

### ì‹±ê¸€í†¤ + Lock (ìŠ¤ë ˆë“œ ì•ˆì „)

```python
import threading

class ThreadSafeSingleton:
    """ìŠ¤ë ˆë“œ ì•ˆì „ ì‹±ê¸€í†¤"""

    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                # Double-checked locking
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

```

---

### 3. Rate Limiter (ì†ë„ ì œí•œ)

### ê°œë…

```
ì‹œê°„ë‹¹ Nê°œê¹Œì§€ë§Œ í—ˆìš©
â†’ API ê³¼ë¶€í•˜ ë°©ì§€
â†’ ì™¸ë¶€ ì„œë¹„ìŠ¤ Rate Limit ëŒ€ì‘
```

### ì–¸ì œ ì‚¬ìš©?

| ì‚¬ìš© O | ì‚¬ìš© X |
| --- | --- |
| ì™¸ë¶€ API í˜¸ì¶œ | ë‚´ë¶€ ì²˜ë¦¬ |
| ì‚¬ìš©ìë³„ ìš”ì²­ ì œí•œ | ì œí•œ ì—†ëŠ” ë¦¬ì†ŒìŠ¤ |
| ë¹„ìš© ì œì–´ |  |

### Token Bucket êµ¬í˜„

```python
import asyncio
import time

class TokenBucketRateLimiter:
    """í† í° ë²„í‚· Rate Limiter"""

    def __init__(self, rate: float, capacity: int):
        """
        Args:
            rate: ì´ˆë‹¹ í† í° ìƒì„± ìˆ˜
            capacity: ë²„í‚· ìµœëŒ€ ìš©ëŸ‰
        """
        self.rate = rate
        self.capacity = capacity
        self.tokens = capacity
        self.last_update = time.monotonic()
        self.lock = asyncio.Lock()

    async def acquire(self, tokens: int = 1) -> bool:
        """í† í° íšë“ ì‹œë„"""
        async with self.lock:
            now = time.monotonic()
            elapsed = now - self.last_update

            # í† í° ì¶©ì „
            self.tokens = min(
                self.capacity,
                self.tokens + elapsed * self.rate
            )
            self.last_update = now

            if self.tokens >= tokens:
                self.tokens -= tokens
                return True
            return False

    async def wait_and_acquire(self, tokens: int = 1):
        """í† í° íšë“ê¹Œì§€ ëŒ€ê¸°"""
        while not await self.acquire(tokens):
            await asyncio.sleep(0.1)

# ì‚¬ìš© - ì´ˆë‹¹ 10ê°œ ìš”ì²­ ì œí•œ
rate_limiter = TokenBucketRateLimiter(rate=10, capacity=10)

async def call_external_api(data):
    await rate_limiter.wait_and_acquire()
    return await external_api.call(data)

```

### Sliding Window êµ¬í˜„

```python
from collections import deque

class SlidingWindowRateLimiter:
    """ìŠ¬ë¼ì´ë”© ìœˆë„ìš° Rate Limiter"""

    def __init__(self, max_requests: int, window_seconds: int):
        """
        Args:
            max_requests: ìœˆë„ìš° ë‚´ ìµœëŒ€ ìš”ì²­ ìˆ˜
            window_seconds: ìœˆë„ìš° í¬ê¸° (ì´ˆ)
        """
        self.max_requests = max_requests
        self.window_seconds = window_seconds
        self.requests: deque = deque()
        self.lock = asyncio.Lock()

    async def is_allowed(self) -> bool:
        """ìš”ì²­ í—ˆìš© ì—¬ë¶€"""
        async with self.lock:
            now = time.monotonic()

            # ìœˆë„ìš° ë°–ì˜ ìš”ì²­ ì œê±°
            while self.requests and self.requests[0] < now - self.window_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(now)
                return True
            return False

# ì‚¬ìš© - ë¶„ë‹¹ 100ê°œ ìš”ì²­ ì œí•œ
limiter = SlidingWindowRateLimiter(max_requests=100, window_seconds=60)

@router.post("/api")
async def api_endpoint(data: Input):
    if not await limiter.is_allowed():
        raise HTTPException(status_code=429, detail="Too Many Requests")

    return await process(data)

```

---

## Part 3: ìˆ˜ëª… ê´€ë¦¬ íŒ¨í„´

### 1. Context Manager

### ê°œë…

```
withë¬¸ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ìë™ ì •ë¦¬
â†’ ì‹œì‘/ì¢…ë£Œ ë¡œì§ ìº¡ìŠí™”
â†’ ì˜ˆì™¸ ë°œìƒí•´ë„ ì •ë¦¬ ë³´ì¥
```

### ë™ê¸° Context Manager

```python
class DatabaseConnection:
    """DB ì—°ê²° Context Manager"""

    def __init__(self, url: str):
        self.url = url
        self.connection = None

    def __enter__(self):
        self.connection = create_connection(self.url)
        return self.connection

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.connection:
            self.connection.close()
        return False  # ì˜ˆì™¸ ì „íŒŒ

# ì‚¬ìš©
with DatabaseConnection(url) as conn:
    conn.execute("SELECT * FROM users")
# ìë™ìœ¼ë¡œ close() í˜¸ì¶œ

```

### ë¹„ë™ê¸° Context Manager

```python
class AsyncHTTPClient:
    """ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.client = None

    async def __aenter__(self):
        self.client = httpx.AsyncClient()
        return self.client

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.client:
            await self.client.aclose()
        return False

# ì‚¬ìš©
async with AsyncHTTPClient() as client:
    response = await client.get("https://api.example.com")

```

### contextlib í™œìš©

```python
from contextlib import asynccontextmanager, contextmanager

@contextmanager
def timer():
    """ì‹¤í–‰ ì‹œê°„ ì¸¡ì •"""
    start = time.time()
    yield
    elapsed = time.time() - start
    print(f"Elapsed: {elapsed:.2f}s")

@asynccontextmanager
async def get_db_session():
    """DB ì„¸ì…˜ ê´€ë¦¬"""
    session = AsyncSession()
    try:
        yield session
        await session.commit()
    except Exception:
        await session.rollback()
        raise
    finally:
        await session.close()

# ì‚¬ìš©
with timer():
    process_data()

async with get_db_session() as session:
    await session.execute(...)

```

---

### 2. FastAPI Lifespan

### ê°œë…

```
ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
â†’ ëª¨ë¸ ë¡œë”©, í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
â†’ ì •ë¦¬ ì‘ì—… ë³´ì¥
```

### êµ¬í˜„

```python
from contextlib import asynccontextmanager
from fastapi import FastAPI

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ìˆ˜ëª…ì£¼ê¸° ê´€ë¦¬"""

    # ===== ì‹œì‘ ì‹œ =====
    print("Starting up...")

    # ëª¨ë¸ ë¡œë”©
    model_manager = ModelManager()
    await model_manager.load()
    app.state.model_manager = model_manager

    # HTTP í´ë¼ì´ì–¸íŠ¸
    http_client = httpx.AsyncClient()
    app.state.http_client = http_client

    # Redis
    redis = await aioredis.from_url(settings.REDIS_URL)
    app.state.redis = redis

    yield  # ì•± ì‹¤í–‰

    # ===== ì¢…ë£Œ ì‹œ =====
    print("Shutting down...")

    await http_client.aclose()
    await redis.close()
    await model_manager.cleanup()

app = FastAPI(lifespan=lifespan)

```

---

## Part 4: íŒ¨í„´ ì„ íƒ ê°€ì´ë“œ

### ìƒí™©ë³„ íŒ¨í„´ ì„ íƒ

```mermaid
flowchart TB
    subgraph Patterns["ìƒí™©ë³„ íŒ¨í„´ ì„ íƒ"]
        P1["í•œ ë²ˆë§Œ ìƒì„±í•˜ê³  ì¬ì‚¬ìš© â†’ Singleton<br/>ì˜ˆ: ëª¨ë¸ ë§¤ë‹ˆì €, ì„¤ì •, LLM í´ë¼ì´ì–¸íŠ¸"]
        P2["ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ê°ì²´ ìƒì„± â†’ Factory<br/>ì˜ˆ: LLM ì–´ëŒ‘í„°, VectorStore ì–´ëŒ‘í„°"]
        P3["ìƒì„± ë¹„ìš©ì´ ë¹„ì‹¸ê³  ì¬ì‚¬ìš© ê°€ëŠ¥ â†’ Pool<br/>ì˜ˆ: DB ì»¤ë„¥ì…˜, HTTP ì»¤ë„¥ì…˜"]
        P4["í•„ìš”í•  ë•Œë§Œ ìƒì„± â†’ Lazy Loading<br/>ì˜ˆ: ì„ íƒì  ëª¨ë¸, ë¬´ê±°ìš´ ì´ˆê¸°í™”"]
        P5["ë™ì‹œ ì ‘ê·¼ Nê°œ ì œí•œ â†’ Semaphore<br/>ì˜ˆ: GPU ì¶”ë¡ , ì™¸ë¶€ API í˜¸ì¶œ"]
        P6["ë™ì‹œ ì ‘ê·¼ 1ê°œë§Œ â†’ Lock<br/>ì˜ˆ: ê³µìœ  ìƒíƒœ ìˆ˜ì •, íŒŒì¼ ì“°ê¸°"]
        P7["ì‹œê°„ë‹¹ Nê°œ ì œí•œ â†’ Rate Limiter<br/>ì˜ˆ: API í˜¸ì¶œ ì œí•œ, ì‚¬ìš©ì ìš”ì²­ ì œí•œ"]
    end
```

### FastAPI ì‹¤ì „ ì¡°í•©

```python
# ì™„ì „í•œ ì˜ˆì‹œ - ëª¨ë“  íŒ¨í„´ ì¡°í•©
from contextlib import asynccontextmanager
from fastapi import FastAPI, Depends, Request
import asyncio

# 1. Singleton - ëª¨ë¸ ë§¤ë‹ˆì €
class ModelManager:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

# 2. Factory - LLM í´ë¼ì´ì–¸íŠ¸
class LLMFactory:
    @staticmethod
    def create(provider: str) -> LLMClient:
        clients = {"openai": OpenAIClient, "anthropic": AnthropicClient}
        return clients[provider]()

# 3. Pool - DB ì»¤ë„¥ì…˜
engine = create_async_engine(DATABASE_URL, pool_size=10)

# 4. Semaphore - GPU ë™ì‹œ ì‚¬ìš© ì œí•œ
gpu_semaphore = asyncio.Semaphore(4)

# 5. Rate Limiter - API í˜¸ì¶œ ì œí•œ
api_limiter = TokenBucketRateLimiter(rate=10, capacity=10)

# Lifespan - ìˆ˜ëª… ê´€ë¦¬
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘
    model_manager = ModelManager()
    await model_manager.load()
    app.state.model_manager = model_manager
    app.state.llm_client = LLMFactory.create(settings.LLM_PROVIDER)

    yield

    # ì¢…ë£Œ
    await model_manager.cleanup()

app = FastAPI(lifespan=lifespan)

# ì˜ì¡´ì„±
def get_model_manager(request: Request) -> ModelManager:
    return request.app.state.model_manager

async def get_db():
    async with AsyncSession(engine) as session:
        yield session

async def acquire_gpu():
    async with gpu_semaphore:
        yield

# ì—”ë“œí¬ì¸íŠ¸
@app.post("/predict")
async def predict(
    data: Input,
    model: ModelManager = Depends(get_model_manager),
    db: AsyncSession = Depends(get_db),
    _: None = Depends(acquire_gpu),  # GPU ìŠ¬ë¡¯ íšë“
):
    # Rate Limit ì²´í¬
    await api_limiter.wait_and_acquire()

    # ì¶”ë¡ 
    result = model.predict(data)

    # DB ì €ì¥
    await db.execute(...)

    return result

```

---

## í•µì‹¬ ì •ë¦¬

| íŒ¨í„´ | ëª©ì  | ì˜ˆì‹œ |
| --- | --- | --- |
| **Singleton** | 1ê°œë§Œ ìƒì„± | ëª¨ë¸, ì„¤ì • |
| **Factory** | ìƒì„± ë¡œì§ ë¶„ë¦¬ | ì–´ëŒ‘í„° ìƒì„± |
| **Pool** | Nê°œ ì¬ì‚¬ìš© | DB ì»¤ë„¥ì…˜ |
| **Lazy Loading** | í•„ìš”ì‹œ ìƒì„± | ì„ íƒì  ëª¨ë¸ |
| **Semaphore** | ë™ì‹œ Nê°œ | GPU ì œí•œ |
| **Lock** | ë™ì‹œ 1ê°œ | ìƒíƒœ ìˆ˜ì • |
| **Rate Limiter** | ì‹œê°„ë‹¹ Nê°œ | API ì œí•œ |
| **Context Manager** | ìë™ ì •ë¦¬ | ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ |

### í•œ ì¤„ ìš”ì•½

```
Singleton = 1ê°œë§Œ, Pool = Nê°œ ì¬ì‚¬ìš©, Semaphore = ë™ì‹œ Nê°œ ì œí•œ
â†’ ìƒí™©ì— ë§ê²Œ ì¡°í•©í•´ì„œ ì‚¬ìš©!
```