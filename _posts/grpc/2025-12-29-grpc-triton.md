---
title: "[gRPC ì‹¤ì „/MLì„œë¹™ 3] Triton Inference Server gRPC"
date: 2025-12-29 02:00:00 +0900
categories: [Tech, gRPC]
tags: [grpc, triton, inference-server, nvidia, ml-serving]
mermaid: true
---

> **ğŸ“š gRPC ì‹œë¦¬ì¦ˆ - Part 3. ì‹¤ì „ êµ¬í˜„ê³¼ ML ì„œë¹™ ì ìš©**
>
> 1. [Python gRPC ì„œë²„/í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„](/posts/grpc-python-impl/)
> 2. [ë¹„ë™ê¸° gRPC (asyncio)](/posts/grpc-async/)
> 3. Triton Inference Server gRPC â† í˜„ì¬ ê¸€
> 4. [vLLM / KServe gRPC ì—°ë™](/posts/grpc-vllm-kserve/)

---

# 3. Triton Inference Server gRPC ì—°ë™

## ì™œ ì´ê±¸ ì•Œì•„ì•¼ í•˜ëŠ”ê°€?

Triton Inference ServerëŠ” NVIDIAì˜ ML ëª¨ë¸ ì„œë¹™ ì†”ë£¨ì…˜ì´ë‹¤.

- ê³ ì„±ëŠ¥ ì¶”ë¡ ì´ í•„ìš”í•  ë•Œ â†’ gRPC ì‚¬ìš©
- ì‹¤ì œ MLOps í˜„ì¥ì—ì„œ ê°€ì¥ ë§ì´ ì“°ëŠ” ì¡°í•©
- KServe, vLLMë„ ë¹„ìŠ·í•œ íŒ¨í„´

gRPCë¥¼ ë°°ì› ìœ¼ë‹ˆ, ì‹¤ì œ ML ì„œë¹™ì— ì ìš©í•´ë³¸ë‹¤.

---

## Triton í†µì‹  ë°©ì‹

### ì§€ì›í•˜ëŠ” í”„ë¡œí† ì½œ

| í”„ë¡œí† ì½œ | í¬íŠ¸ (ê¸°ë³¸) | ìš©ë„ |
| --- | --- | --- |
| HTTP/REST | 8000 | ë””ë²„ê¹…, ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ |
| gRPC | 8001 | í”„ë¡œë•ì…˜, ê³ ì„±ëŠ¥ |
| Metrics | 8002 | Prometheus ë©”íŠ¸ë¦­ |

### ì–¸ì œ gRPCë¥¼ ì“°ë‚˜?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      í”„ë¡œí† ì½œ ì„ íƒ ê¸°ì¤€                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   REST (8000)                                                       â”‚
â”‚   â€¢ curlë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸                                               â”‚
â”‚   â€¢ ë””ë²„ê¹…, ê°œë°œ í™˜ê²½                                                â”‚
â”‚   â€¢ íŠ¸ë˜í”½ ì ì€ ì„œë¹„ìŠ¤                                               â”‚
â”‚                                                                     â”‚
â”‚   gRPC (8001)                                                       â”‚
â”‚   â€¢ í”„ë¡œë•ì…˜ í™˜ê²½                                                    â”‚
â”‚   â€¢ ëŒ€ìš©ëŸ‰ í…ì„œ ì „ì†¡                                                 â”‚
â”‚   â€¢ ë‚®ì€ ì§€ì—°ì‹œê°„ í•„ìš”                                               â”‚
â”‚   â€¢ ë°°ì¹˜ ì¶”ë¡                                                         â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

---

## Triton gRPC í´ë¼ì´ì–¸íŠ¸

### ì„¤ì¹˜

```bash
pip install tritonclient[grpc]

# ë˜ëŠ” ì „ì²´ ì„¤ì¹˜
pip install tritonclient[all]

```

### ê¸°ë³¸ êµ¬ì¡°

```python
import tritonclient.grpc as grpcclient

# í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = grpcclient.InferenceServerClient(
    url="localhost:8001",  # gRPC í¬íŠ¸
    verbose=False
)

# ì„œë²„ ìƒíƒœ í™•ì¸
if client.is_server_live():
    print("Server is live")

if client.is_server_ready():
    print("Server is ready")

```

---

## ì¶”ë¡  ìš”ì²­ ë³´ë‚´ê¸°

### 1. ì…ë ¥ ë°ì´í„° ì¤€ë¹„

```python
import numpy as np
import tritonclient.grpc as grpcclient

# ì…ë ¥ ë°ì´í„° (ì˜ˆ: ì´ë¯¸ì§€ ë¶„ë¥˜)
input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)

# InferInput ê°ì²´ ìƒì„±
inputs = [
    grpcclient.InferInput(
        name="input",           # ëª¨ë¸ì˜ ì…ë ¥ ì´ë¦„
        shape=[1, 3, 224, 224], # ì…ë ¥ shape
        datatype="FP32"         # ë°ì´í„° íƒ€ì…
    )
]

# ë°ì´í„° ì„¤ì •
inputs[0].set_data_from_numpy(input_data)

```

### 2. ì¶œë ¥ ì„¤ì •

```python
# ì¶œë ¥ ì •ì˜
outputs = [
    grpcclient.InferRequestedOutput(
        name="output",  # ëª¨ë¸ì˜ ì¶œë ¥ ì´ë¦„
    )
]

```

### 3. ì¶”ë¡  ìš”ì²­

```python
# ì¶”ë¡  ì‹¤í–‰
response = client.infer(
    model_name="resnet50",
    inputs=inputs,
    outputs=outputs,
)

# ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
output_data = response.as_numpy("output")
print(f"Output shape: {output_data.shape}")
print(f"Prediction: {np.argmax(output_data)}")

```

---

## ì „ì²´ ì˜ˆì‹œ ì½”ë“œ

### ì´ë¯¸ì§€ ë¶„ë¥˜ ì¶”ë¡ 

```python
import numpy as np
import tritonclient.grpc as grpcclient
from tritonclient.utils import InferenceServerException

class TritonClient:
    """Triton gRPC í´ë¼ì´ì–¸íŠ¸ ë˜í¼"""

    def __init__(self, url="localhost:8001"):
        self.client = grpcclient.InferenceServerClient(
            url=url,
            verbose=False
        )

    def check_health(self):
        """ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            live = self.client.is_server_live()
            ready = self.client.is_server_ready()
            return {"live": live, "ready": ready}
        except InferenceServerException as e:
            return {"error": str(e)}

    def get_model_metadata(self, model_name):
        """ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
        try:
            metadata = self.client.get_model_metadata(model_name)
            return {
                "name": metadata.name,
                "versions": metadata.versions,
                "inputs": [
                    {"name": inp.name, "shape": inp.shape, "datatype": inp.datatype}
                    for inp in metadata.inputs
                ],
                "outputs": [
                    {"name": out.name, "shape": out.shape, "datatype": out.datatype}
                    for out in metadata.outputs
                ],
            }
        except InferenceServerException as e:
            return {"error": str(e)}

    def infer(self, model_name, input_data, input_name="input", output_name="output"):
        """ì¶”ë¡  ì‹¤í–‰"""
        try:
            # ì…ë ¥ ì„¤ì •
            inputs = [
                grpcclient.InferInput(
                    name=input_name,
                    shape=input_data.shape,
                    datatype="FP32"
                )
            ]
            inputs[0].set_data_from_numpy(input_data.astype(np.float32))

            # ì¶œë ¥ ì„¤ì •
            outputs = [
                grpcclient.InferRequestedOutput(name=output_name)
            ]

            # ì¶”ë¡ 
            response = self.client.infer(
                model_name=model_name,
                inputs=inputs,
                outputs=outputs,
            )

            return response.as_numpy(output_name)

        except InferenceServerException as e:
            print(f"Inference error: {e}")
            return None

def main():
    client = TritonClient("localhost:8001")

    # 1. ì„œë²„ ìƒíƒœ í™•ì¸
    print("=== Server Health ===")
    health = client.check_health()
    print(health)

    # 2. ëª¨ë¸ ë©”íƒ€ë°ì´í„° í™•ì¸
    print("\n=== Model Metadata ===")
    metadata = client.get_model_metadata("resnet50")
    print(metadata)

    # 3. ì¶”ë¡  ì‹¤í–‰
    print("\n=== Inference ===")
    input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)
    output = client.infer("resnet50", input_data)

    if output is not None:
        print(f"Output shape: {output.shape}")
        print(f"Top prediction: {np.argmax(output)}")

if __name__ == "__main__":
    main()

```

---

## ë°°ì¹˜ ì¶”ë¡ 

### ì—¬ëŸ¬ ìƒ˜í”Œ í•œ ë²ˆì— ì²˜ë¦¬

```python
def batch_infer(client, model_name, batch_data):
    """ë°°ì¹˜ ì¶”ë¡ """

    # batch_data shape: (batch_size, 3, 224, 224)
    batch_size = batch_data.shape[0]

    inputs = [
        grpcclient.InferInput(
            name="input",
            shape=batch_data.shape,  # (N, 3, 224, 224)
            datatype="FP32"
        )
    ]
    inputs[0].set_data_from_numpy(batch_data)

    outputs = [grpcclient.InferRequestedOutput(name="output")]

    response = client.infer(
        model_name=model_name,
        inputs=inputs,
        outputs=outputs,
    )

    return response.as_numpy("output")  # (N, num_classes)

# ì‚¬ìš© ì˜ˆì‹œ
batch_size = 8
batch_data = np.random.randn(batch_size, 3, 224, 224).astype(np.float32)

results = batch_infer(client, "resnet50", batch_data)
print(f"Batch results shape: {results.shape}")  # (8, 1000)

```

---

## ë¹„ë™ê¸° ì¶”ë¡ 

### AsyncIO ê¸°ë°˜

```python
import asyncio
import tritonclient.grpc.aio as grpcclient_aio

async def async_infer(url, model_name, input_data):
    """ë¹„ë™ê¸° ì¶”ë¡ """

    # ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸
    client = grpcclient_aio.InferenceServerClient(url=url)

    inputs = [
        grpcclient_aio.InferInput(
            name="input",
            shape=input_data.shape,
            datatype="FP32"
        )
    ]
    inputs[0].set_data_from_numpy(input_data)

    outputs = [grpcclient_aio.InferRequestedOutput(name="output")]

    # ë¹„ë™ê¸° ì¶”ë¡ 
    response = await client.infer(
        model_name=model_name,
        inputs=inputs,
        outputs=outputs,
    )

    await client.close()

    return response.as_numpy("output")

async def main():
    url = "localhost:8001"
    model_name = "resnet50"

    # ì—¬ëŸ¬ ìš”ì²­ ë™ì‹œ ì‹¤í–‰
    tasks = []
    for i in range(10):
        input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)
        tasks.append(async_infer(url, model_name, input_data))

    results = await asyncio.gather(*tasks)
    print(f"Completed {len(results)} inferences")

if __name__ == "__main__":
    asyncio.run(main())

```

---

## ìŠ¤íŠ¸ë¦¬ë° ì¶”ë¡  (LLM)

### Decoupled ëª¨ë¸ (í† í° ìŠ¤íŠ¸ë¦¬ë°)

```python
import queue
import tritonclient.grpc as grpcclient

def stream_callback(result, error):
    """ìŠ¤íŠ¸ë¦¬ë° ì½œë°±"""
    if error:
        print(f"Error: {error}")
    else:
        output = result.as_numpy("output_ids")
        print(f"Token: {output}")

def stream_infer(client, model_name, input_text):
    """ìŠ¤íŠ¸ë¦¬ë° ì¶”ë¡  (LLM)"""

    # ì…ë ¥ ì„¤ì •
    input_data = np.array([[input_text]], dtype=object)

    inputs = [
        grpcclient.InferInput(
            name="text_input",
            shape=[1, 1],
            datatype="BYTES"
        )
    ]
    inputs[0].set_data_from_numpy(input_data)

    outputs = [grpcclient.InferRequestedOutput(name="output_ids")]

    # ìŠ¤íŠ¸ë¦¬ë° ì¶”ë¡  ì‹œì‘
    client.start_stream(callback=stream_callback)

    client.async_stream_infer(
        model_name=model_name,
        inputs=inputs,
        outputs=outputs,
    )

    # ì™„ë£Œ ëŒ€ê¸°
    client.stop_stream()

```

---

## FastAPI + Triton í†µí•©

### Gateway íŒ¨í„´

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import numpy as np
import tritonclient.grpc as grpcclient

app = FastAPI()

# Triton í´ë¼ì´ì–¸íŠ¸ (ì‹±ê¸€í†¤)
triton_client = grpcclient.InferenceServerClient(url="triton-server:8001")

class PredictRequest(BaseModel):
    data: list  # ì…ë ¥ ë°ì´í„°

class PredictResponse(BaseModel):
    prediction: int
    confidence: float

@app.get("/health")
async def health():
    """í—¬ìŠ¤ ì²´í¬"""
    try:
        live = triton_client.is_server_live()
        ready = triton_client.is_server_ready()
        return {"triton_live": live, "triton_ready": ready}
    except Exception as e:
        raise HTTPException(status_code=503, detail=str(e))

@app.post("/predict", response_model=PredictResponse)
async def predict(request: PredictRequest):
    """ì¶”ë¡  API"""
    try:
        # ì…ë ¥ ë³€í™˜
        input_data = np.array(request.data, dtype=np.float32)
        input_data = input_data.reshape(1, 3, 224, 224)

        # Triton ì…ë ¥ ì„¤ì •
        inputs = [
            grpcclient.InferInput("input", input_data.shape, "FP32")
        ]
        inputs[0].set_data_from_numpy(input_data)

        outputs = [grpcclient.InferRequestedOutput("output")]

        # ì¶”ë¡ 
        response = triton_client.infer(
            model_name="resnet50",
            inputs=inputs,
            outputs=outputs,
        )

        # ê²°ê³¼ ì²˜ë¦¬
        output = response.as_numpy("output")
        prediction = int(np.argmax(output))
        confidence = float(np.max(output))

        return PredictResponse(
            prediction=prediction,
            confidence=confidence
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

```

---

## ì—ëŸ¬ ì²˜ë¦¬

### ê³µí†µ ì—ëŸ¬ íŒ¨í„´

```python
from tritonclient.utils import InferenceServerException

def safe_infer(client, model_name, input_data):
    """ì—ëŸ¬ ì²˜ë¦¬ê°€ í¬í•¨ëœ ì¶”ë¡ """

    try:
        # ëª¨ë¸ ì¤€ë¹„ ìƒíƒœ í™•ì¸
        if not client.is_model_ready(model_name):
            return {"error": f"Model {model_name} is not ready"}

        # ì¶”ë¡  ì‹¤í–‰
        inputs = [grpcclient.InferInput("input", input_data.shape, "FP32")]
        inputs[0].set_data_from_numpy(input_data)
        outputs = [grpcclient.InferRequestedOutput("output")]

        response = client.infer(
            model_name=model_name,
            inputs=inputs,
            outputs=outputs,
            client_timeout=10.0,  # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        )

        return {"success": True, "output": response.as_numpy("output")}

    except InferenceServerException as e:
        # Triton ì„œë²„ ì—ëŸ¬
        return {"error": f"Triton error: {e.message()}"}

    except Exception as e:
        # ê¸°íƒ€ ì—ëŸ¬
        return {"error": f"Unknown error: {str(e)}"}

```

### ì¬ì‹œë„ ë¡œì§

```python
import time

def infer_with_retry(client, model_name, input_data, max_retries=3):
    """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ì¶”ë¡ """

    for attempt in range(max_retries):
        try:
            inputs = [grpcclient.InferInput("input", input_data.shape, "FP32")]
            inputs[0].set_data_from_numpy(input_data)
            outputs = [grpcclient.InferRequestedOutput("output")]

            response = client.infer(
                model_name=model_name,
                inputs=inputs,
                outputs=outputs,
            )

            return response.as_numpy("output")

        except InferenceServerException as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # ì§€ìˆ˜ ë°±ì˜¤í”„
                print(f"Retry {attempt + 1}/{max_retries} after {wait_time}s")
                time.sleep(wait_time)
            else:
                raise

```

---

## K8s í™˜ê²½ ì„¤ì •

### ì„œë¹„ìŠ¤ ì—°ê²°

```yaml
# triton-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: triton-server
spec:
  selector:
    app: triton
  ports:
    - name: http
      port: 8000
      targetPort: 8000
    - name: grpc
      port: 8001
      targetPort: 8001
    - name: metrics
      port: 8002
      targetPort: 8002

```

```python
# K8s ë‚´ë¶€ì—ì„œ ì—°ê²°
client = grpcclient.InferenceServerClient(
    url="triton-server:8001",  # ì„œë¹„ìŠ¤ ì´ë¦„ìœ¼ë¡œ ì ‘ê·¼
)

```

---

## ì„±ëŠ¥ íŒ

### 1. ì—°ê²° ì¬ì‚¬ìš©

```python
# âŒ ë§¤ë²ˆ ìƒˆ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
def infer(data):
    client = grpcclient.InferenceServerClient(url="localhost:8001")
    # ...

# âœ… í´ë¼ì´ì–¸íŠ¸ ì¬ì‚¬ìš©
client = grpcclient.InferenceServerClient(url="localhost:8001")

def infer(data):
    # ê¸°ì¡´ client ì‚¬ìš©
    # ...

```

### 2. ë°°ì¹˜ ì²˜ë¦¬

```python
# âŒ ê°œë³„ ìš”ì²­
for data in dataset:
    result = infer(data.reshape(1, ...))

# âœ… ë°°ì¹˜ ìš”ì²­
batch = np.stack(dataset)
results = infer(batch)  # í•œ ë²ˆì— ì²˜ë¦¬

```

### 3. ë¹„ë™ê¸° ì²˜ë¦¬

```python
# âŒ ìˆœì°¨ ì²˜ë¦¬
results = []
for data in inputs:
    results.append(await infer(data))

# âœ… ë™ì‹œ ì²˜ë¦¬
tasks = [infer(data) for data in inputs]
results = await asyncio.gather(*tasks)

```

---

## í•µì‹¬ ì •ë¦¬

### Triton gRPC ì‚¬ìš© íë¦„

1. í´ë¼ì´ì–¸íŠ¸ ìƒì„±: `InferenceServerClient(url="host:8001")`
2. ì…ë ¥ ì„¤ì •: `InferInput` + `set_data_from_numpy()`
3. ì¶œë ¥ ì„¤ì •: `InferRequestedOutput`
4. ì¶”ë¡  ì‹¤í–‰: `client.infer()`
5. ê²°ê³¼ ì¶”ì¶œ: `response.as_numpy()`

### REST vs gRPC ì„ íƒ

| ìƒí™© | ì„ íƒ |
| --- | --- |
| ê°œë°œ/ë””ë²„ê¹… | REST (8000) |
| í”„ë¡œë•ì…˜ | gRPC (8001) |
| ëŒ€ìš©ëŸ‰ í…ì„œ | gRPC |
| ë°°ì¹˜ ì¶”ë¡  | gRPC |
| ìŠ¤íŠ¸ë¦¬ë° (LLM) | gRPC |

| ìƒí™© | ì¶”ì²œ | ì´ìœ  |
| --- | --- | --- |
| ë””ë²„ê¹…/í…ŒìŠ¤íŠ¸ | HTTP | curlë¡œ ë°”ë¡œ í…ŒìŠ¤íŠ¸ |
| ë‹¨ì¼ ìš”ì²­, ì‘ì€ ë°ì´í„° | HTTP | ì°¨ì´ ë¯¸ë¯¸, ê°œë°œ í¸í•¨ |
| íŒ€ì´ gRPC ê²½í—˜ ì—†ìŒ | HTTP | ëŸ¬ë‹ì»¤ë¸Œ ì—†ìŒ |
| ë‹¤ë¥¸ íŒ€ê³¼ í˜‘ì—… | HTTP | ìŠ¤í™ ê³µìœ  ì‰¬ì›€ |
| ëŒ€ìš©ëŸ‰ í…ì„œ/ë°°ì¹˜ ì¶”ë¡  | gRPC | í˜ì´ë¡œë“œ íš¨ìœ¨ì  ì „ì†¡ |
| ë°˜ë³µ í˜¸ì¶œ ë§ìŒ | gRPC | ì—°ê²° ì¬ì‚¬ìš©, í—¤ë” ì••ì¶• |
| LLM í† í° ìŠ¤íŠ¸ë¦¬ë° | gRPC | ë„¤ì´í‹°ë¸Œ ìŠ¤íŠ¸ë¦¬ë° ì§€ì› |
| ê·¹í•œì˜ ì €ì§€ì—° í•„ìš” | gRPC | ë°”ì´ë„ˆë¦¬ ì§ë ¬í™” |

### ì£¼ìš” í´ë˜ìŠ¤

| í´ë˜ìŠ¤ | ìš©ë„ |
| --- | --- |
| `InferenceServerClient` | ë™ê¸° í´ë¼ì´ì–¸íŠ¸ |
| `InferenceServerClient` (aio) | ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ |
| `InferInput` | ì…ë ¥ í…ì„œ ì •ì˜ |
| `InferRequestedOutput` | ì¶œë ¥ í…ì„œ ì •ì˜ |

---

## ê¸°íƒ€ ë‚´ìš©

## HTTPê°€ ìœ ë¦¬í•œ ê²½ìš°

| ìƒí™© | ì´ìœ  |
| --- | --- |
| **ë””ë²„ê¹…/í…ŒìŠ¤íŠ¸** | curlë¡œ ë°”ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ |
| **ë‹¨ì¼ ìš”ì²­** | í”„ë¡œí† ì½œ ì˜¤ë²„í—¤ë“œ ì°¨ì´ ë¯¸ë¯¸ |
| **ì‘ì€ ì…ë ¥ ë°ì´í„°** | í…ìŠ¤íŠ¸ ë¶„ë¥˜, ì‘ì€ ì´ë¯¸ì§€ ë“± |
| **íŒ€ì´ gRPC ê²½í—˜ ì—†ìŒ** | ëŸ¬ë‹ì»¤ë¸Œ ì—†ì´ ë¹ ë¥¸ ê°œë°œ |
| **ë‹¤ë¥¸ íŒ€ê³¼ í˜‘ì—…** | ìŠ¤í™ ê³µìœ ê°€ ì‰¬ì›€ |

---

## gRPCê°€ ìœ ë¦¬í•œ ê²½ìš°

| ìƒí™© | ì´ìœ  |
| --- | --- |
| **ëŒ€ìš©ëŸ‰ í…ì„œ** | ì´ë¯¸ì§€ ë°°ì¹˜, ê³ í•´ìƒë„ |
| **ë°˜ë³µ í˜¸ì¶œì´ ë§ìŒ** | ì—°ê²° ì¬ì‚¬ìš©, í—¤ë” ì••ì¶• |
| **ë°°ì¹˜ ì¶”ë¡ ** | í° í˜ì´ë¡œë“œ íš¨ìœ¨ì  ì „ì†¡ |
| **ìŠ¤íŠ¸ë¦¬ë° (LLM)** | í† í° ë‹¨ìœ„ ì‘ë‹µ |
| **ê·¹í•œì˜ ì €ì§€ì—° í•„ìš”** | ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ |