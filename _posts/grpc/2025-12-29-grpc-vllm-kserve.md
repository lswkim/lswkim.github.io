---
title: "[gRPC μ‹¤μ „/MLμ„λΉ™ 4] vLLM / KServe gRPC μ—°λ™"
date: 2025-12-29 03:00:00 +0900
categories: [Tech, gRPC]
tags: [grpc, vllm, kserve, llm, ml-serving]
mermaid: true
---

> **π“ gRPC μ‹λ¦¬μ¦ - Part 3. μ‹¤μ „ κµ¬ν„κ³Ό ML μ„λΉ™ μ μ©**
>
> 1. [Python gRPC μ„λ²„/ν΄λΌμ΄μ–ΈνΈ κµ¬ν„](/posts/grpc-python-impl/)
> 2. [λΉ„λ™κΈ° gRPC (asyncio)](/posts/grpc-async/)
> 3. [Triton Inference Server gRPC](/posts/grpc-triton/)
> 4. vLLM / KServe gRPC μ—°λ™ β† ν„μ¬ κΈ€

---

# 4. vLLM gRPC μ—°λ™

## vLLMμ΄λ€?

vLLMμ€ LLM μ¶”λ΅ μ— νΉν™”λ κ³ μ„±λ¥ μ„λΉ™ μ—”μ§„μ΄λ‹¤.

- PagedAttentionμΌλ΅ λ©”λ¨λ¦¬ ν¨μ¨ κ·Ήλ€ν™”
- Continuous BatchingμΌλ΅ μ²λ¦¬λ‰ ν–¥μƒ
- OpenAI νΈν™ API μ κ³µ

---

## vLLM ν†µμ‹  λ°©μ‹

| λ°©μ‹ | ν¬νΈ (κΈ°λ³Έ) | νΉμ§• |
| --- | --- | --- |
| OpenAI νΈν™ REST | 8000 | `/v1/completions`, `/v1/chat/completions` |
| gRPC | λ³„λ„ μ„¤μ • | Triton λ°±μ—”λ“ μ‚¬μ© μ‹ |

### vLLMμ νΉμ΄μ 

vLLMμ€ κΈ°λ³Έμ μΌλ΅ **OpenAI νΈν™ REST API**λ¥Ό μ κ³µν•λ‹¤.

```bash
# vLLM μ„λ²„ μ‹¤ν–‰
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --port 8000

```

gRPCλ¥Ό μ“°λ ¤λ©΄:

1. **Triton + vLLM λ°±μ—”λ“** μ΅°ν•©
2. **μ§μ ‘ gRPC λνΌ κµ¬ν„**

---

## λ°©λ²• 1: OpenAI νΈν™ API (REST)

### κ°€μ¥ κ°„λ‹¨ν• λ°©λ²•

```python
import requests

response = requests.post(
    "http://localhost:8000/v1/completions",
    json={
        "model": "meta-llama/Llama-2-7b-chat-hf",
        "prompt": "μ•λ…•ν•μ„Έμ”",
        "max_tokens": 100,
        "temperature": 0.7,
    }
)

print(response.json()["choices"][0]["text"])

```

### OpenAI SDK μ‚¬μ©

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"  # vLLMμ€ API key κ²€μ¦ μ• ν•¨
)

response = client.completions.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    prompt="μ•λ…•ν•μ„Έμ”",
    max_tokens=100,
)

print(response.choices[0].text)

```

### μ¤νΈλ¦¬λ° (SSE)

```python
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8000/v1", api_key="dummy")

stream = client.chat.completions.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    messages=[{"role": "user", "content": "μ•λ…•ν•μ„Έμ”"}],
    max_tokens=100,
    stream=True,  # μ¤νΈλ¦¬λ° ν™μ„±ν™”
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)

```

---

## λ°©λ²• 2: Triton + vLLM λ°±μ—”λ“ (gRPC)

### κµ¬μ΅°

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                                                                     β”‚
β”‚   Client β”€β”€β”€ gRPC (8001) β”€β”€β”€β–Ί Triton Server β”€β”€β”€β–Ί vLLM Backend       β”‚
β”‚                                                                     β”‚
β”‚   β€Ά Tritonμ΄ gRPC μΈν„°νμ΄μ¤ μ κ³µ                                     β”‚
β”‚   β€Ά vLLMμ΄ μ‹¤μ  LLM μ¶”λ΅  μν–‰                                        β”‚
β”‚                                                                     β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

```

### λ¨λΈ μ €μ¥μ† κµ¬μ΅°

```
model_repository/
β””β”€β”€ vllm_model/
    β”β”€β”€ 1/
    β”‚   β””β”€β”€ model.json
    β””β”€β”€ config.pbtxt

```

### config.pbtxt

```
name: "vllm_model"
backend: "vllm"
max_batch_size: 0

input [
  {
    name: "text_input"
    data_type: TYPE_STRING
    dims: [ 1 ]
  },
  {
    name: "stream"
    data_type: TYPE_BOOL
    dims: [ 1 ]
    optional: true
  }
]

output [
  {
    name: "text_output"
    data_type: TYPE_STRING
    dims: [ -1 ]
  }
]

parameters {
  key: "model"
  value: { string_value: "meta-llama/Llama-2-7b-chat-hf" }
}

parameters {
  key: "gpu_memory_utilization"
  value: { string_value: "0.9" }
}

```

### gRPC ν΄λΌμ΄μ–ΈνΈ μ½”λ“

```python
import numpy as np
import tritonclient.grpc as grpcclient

def generate(prompt, max_tokens=100, stream=False):
    client = grpcclient.InferenceServerClient(url="localhost:8001")

    # μ…λ ¥ μ„¤μ •
    text_input = np.array([[prompt]], dtype=object)
    stream_input = np.array([[stream]], dtype=bool)

    inputs = [
        grpcclient.InferInput("text_input", [1, 1], "BYTES"),
        grpcclient.InferInput("stream", [1, 1], "BOOL"),
    ]
    inputs[0].set_data_from_numpy(text_input)
    inputs[1].set_data_from_numpy(stream_input)

    outputs = [grpcclient.InferRequestedOutput("text_output")]

    # μ¶”λ΅ 
    response = client.infer(
        model_name="vllm_model",
        inputs=inputs,
        outputs=outputs,
    )

    return response.as_numpy("text_output")[0].decode("utf-8")

# μ‚¬μ©
result = generate("λ€ν•λ―Όκµ­μ μλ„λ”?")
print(result)

```

### μ¤νΈλ¦¬λ° (gRPC)

```python
import queue
import numpy as np
import tritonclient.grpc as grpcclient

def stream_callback(result, error):
    if error:
        print(f"Error: {error}")
    else:
        output = result.as_numpy("text_output")
        token = output[0].decode("utf-8")
        print(token, end="", flush=True)

def stream_generate(prompt):
    client = grpcclient.InferenceServerClient(url="localhost:8001")

    text_input = np.array([[prompt]], dtype=object)
    stream_input = np.array([[True]], dtype=bool)

    inputs = [
        grpcclient.InferInput("text_input", [1, 1], "BYTES"),
        grpcclient.InferInput("stream", [1, 1], "BOOL"),
    ]
    inputs[0].set_data_from_numpy(text_input)
    inputs[1].set_data_from_numpy(stream_input)

    outputs = [grpcclient.InferRequestedOutput("text_output")]

    # μ¤νΈλ¦¬λ° μ‹μ‘
    client.start_stream(callback=stream_callback)

    client.async_stream_infer(
        model_name="vllm_model",
        inputs=inputs,
        outputs=outputs,
    )

    client.stop_stream()

# μ‚¬μ©
stream_generate("μ¤λ λ‚ μ”¨κ°€ μΆ‹λ„¤μ”.")

```

---

## λ°©λ²• 3: μ§μ ‘ gRPC μ„λ²„ κµ¬ν„

### vLLM + μ»¤μ¤ν…€ gRPC λνΌ

```protobuf
// llm.proto
syntax = "proto3";

package llm;

message GenerateRequest {
    string prompt = 1;
    int32 max_tokens = 2;
    float temperature = 3;
}

message GenerateResponse {
    string text = 1;
}

message Token {
    string text = 1;
    bool is_finished = 2;
}

service LLMService {
    rpc Generate(GenerateRequest) returns (GenerateResponse);
    rpc StreamGenerate(GenerateRequest) returns (stream Token);
}

```

### μ„λ²„ κµ¬ν„

```python
import grpc
from concurrent import futures
from vllm import LLM, SamplingParams

from generated import llm_pb2, llm_pb2_grpc

class LLMServiceServicer(llm_pb2_grpc.LLMServiceServicer):

    def __init__(self, model_name):
        self.llm = LLM(model=model_name)

    def Generate(self, request, context):
        """λ‹¨μΌ μ‘λ‹µ"""
        sampling_params = SamplingParams(
            max_tokens=request.max_tokens,
            temperature=request.temperature,
        )

        outputs = self.llm.generate([request.prompt], sampling_params)
        text = outputs[0].outputs[0].text

        return llm_pb2.GenerateResponse(text=text)

    def StreamGenerate(self, request, context):
        """μ¤νΈλ¦¬λ° μ‘λ‹µ"""
        sampling_params = SamplingParams(
            max_tokens=request.max_tokens,
            temperature=request.temperature,
        )

        # vLLM μ¤νΈλ¦¬λ° μƒμ„±
        for output in self.llm.generate(
            [request.prompt],
            sampling_params,
            stream=True
        ):
            token = output.outputs[0].text
            is_finished = output.finished

            yield llm_pb2.Token(text=token, is_finished=is_finished)

def serve():
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=4))

    llm_pb2_grpc.add_LLMServiceServicer_to_server(
        LLMServiceServicer("meta-llama/Llama-2-7b-chat-hf"),
        server
    )

    server.add_insecure_port("[::]:50051")
    server.start()
    print("LLM gRPC server started on 50051")
    server.wait_for_termination()

if __name__ == "__main__":
    serve()

```

### ν΄λΌμ΄μ–ΈνΈ κµ¬ν„

```python
import grpc
from generated import llm_pb2, llm_pb2_grpc

def generate(prompt, max_tokens=100):
    channel = grpc.insecure_channel("localhost:50051")
    stub = llm_pb2_grpc.LLMServiceStub(channel)

    response = stub.Generate(
        llm_pb2.GenerateRequest(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=0.7,
        )
    )

    return response.text

def stream_generate(prompt, max_tokens=100):
    channel = grpc.insecure_channel("localhost:50051")
    stub = llm_pb2_grpc.LLMServiceStub(channel)

    for token in stub.StreamGenerate(
        llm_pb2.GenerateRequest(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=0.7,
        )
    ):
        print(token.text, end="", flush=True)
        if token.is_finished:
            break

# μ‚¬μ©
print(generate("μ•λ…•ν•μ„Έμ”"))
print()
stream_generate("μ¤λ μ μ‹¬ λ­ λ¨Ήμ„κΉ?")

```

---

## λ°©λ²• λΉ„κµ

| λ°©λ²• | μ¥μ  | λ‹¨μ  |
| --- | --- | --- |
| **OpenAI νΈν™ REST** | κ°„λ‹¨, ν‘μ¤€ SDK μ‚¬μ© | gRPC λ€λΉ„ μ¤λ²„ν—¤λ“ |
| **Triton + vLLM** | Triton μƒνƒκ³„ ν™μ©, κ³ μ„±λ¥ | μ„¤μ • λ³µμ΅ |
| **μ§μ ‘ gRPC κµ¬ν„** | μ™„μ „ν• μ»¤μ¤ν„°λ§μ΄μ§• | κ°λ°/μ μ§€λ³΄μ λ¶€λ‹΄ |

---

## μ–Έμ  λ­ μ“ΈκΉ?

| μƒν™© | μ¶”μ² |
| --- | --- |
| λΉ λ¥Έ κ°λ°/PoC | OpenAI νΈν™ REST |
| μ΄λ―Έ Triton μ‚¬μ© μ¤‘ | Triton + vLLM λ°±μ—”λ“ |
| κ·Ήν•μ μ»¤μ¤ν„°λ§μ΄μ§• ν•„μ” | μ§μ ‘ gRPC κµ¬ν„ |
| λ‹¨μ μ±„ν… μ„λΉ„μ¤ | OpenAI νΈν™ REST |
| λ€κ·λ¨ ν”„λ΅λ•μ… | Triton + vLLM λλ” μ§μ ‘ κµ¬ν„ |

---

## FastAPI + vLLM ν†µν•© μμ‹

### REST Gateway (κ°€μ¥ μ‹¤μ©μ )

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from openai import OpenAI
import json

app = FastAPI()

vllm_client = OpenAI(
    base_url="http://vllm-server:8000/v1",
    api_key="dummy"
)

class ChatRequest(BaseModel):
    message: str
    max_tokens: int = 100

@app.post("/chat")
async def chat(request: ChatRequest):
    """μΌλ° μ‘λ‹µ"""
    response = vllm_client.chat.completions.create(
        model="meta-llama/Llama-2-7b-chat-hf",
        messages=[{"role": "user", "content": request.message}],
        max_tokens=request.max_tokens,
    )
    return {"response": response.choices[0].message.content}

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """μ¤νΈλ¦¬λ° μ‘λ‹µ"""

    def generate():
        stream = vllm_client.chat.completions.create(
            model="meta-llama/Llama-2-7b-chat-hf",
            messages=[{"role": "user", "content": request.message}],
            max_tokens=request.max_tokens,
            stream=True,
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield f"data: {json.dumps({'text': chunk.choices[0].delta.content})}\n\n"

        yield "data: [DONE]\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")

```

---

## ν•µμ‹¬ μ •λ¦¬

### vLLM ν†µμ‹  λ°©μ‹

- **κΈ°λ³Έ**: OpenAI νΈν™ REST API
- **gRPC**: Triton λ°±μ—”λ“ λλ” μ§μ ‘ κµ¬ν„

### μ„ νƒ κΈ°μ¤€

| μƒν™© | μ„ νƒ |
| --- | --- |
| κ°„λ‹¨ν• μ„λΉ„μ¤ | OpenAI νΈν™ REST |
| Triton ν™κ²½ | Triton + vLLM |
| μ™„μ „ν• μ μ–΄ ν•„μ” | μ§μ ‘ gRPC κµ¬ν„ |

### μ¤νΈλ¦¬λ°

- REST: SSE (Server-Sent Events)
- gRPC: Server Streaming RPC

---

## κΈ°νƒ€λ‚΄μ©

## vLLMμ— gRPCκ°€ ν•„μ”ν•κ°€

### λ¶ν•„μ”

| νΉμ§• | μ„¤λ… |
| --- | --- |
| **κΈ°λ³Έμ΄ REST** | vLLMμ΄ OpenAI νΈν™ APIλ¥Ό λ„¤μ΄ν‹°λΈλ΅ μ κ³µ |
| **μ¤νΈλ¦¬λ° μ§€μ›** | RESTμ—μ„λ„ SSEλ΅ ν† ν° μ¤νΈλ¦¬λ° κ°€λ¥ |
| **μƒνƒκ³„** | OpenAI SDK κ·Έλ€λ΅ μ‚¬μ© κ°€λ¥ |
| **μ„±λ¥ μ¶©λ¶„** | LLM μ¶”λ΅  μμ²΄κ°€ λ³‘λ©, ν”„λ΅ν† μ½ μ°¨μ΄ λ―Έλ―Έ |

---

### vLLM vs Triton μ°¨μ΄

|  | vLLM | Triton |
| --- | --- | --- |
| **κΈ°λ³Έ ν”„λ΅ν† μ½** | REST (OpenAI νΈν™) | REST + gRPC λ‘ λ‹¤ |
| **gRPC ν•„μ”μ„±** | λ‚®μ | λ†’μ (λ€μ©λ‰ ν…μ„) |
| **μ£Ό μ‚¬μ©μ²** | LLM μ „μ© | λ²”μ© λ¨λΈ μ„λΉ™ |

---

### μ™ Tritonμ€ gRPCκ°€ μ λ¦¬ν•κ°€?

```
Triton: μ΄λ―Έμ§€ λ°°μΉ 32μ¥ (15MB) β†’ gRPC ν¨κ³Ό νΌ
vLLM:   ν…μ¤νΈ ν”„λ΅¬ν”„νΈ (μ KB) β†’ RESTλ΅ μ¶©λ¶„

```

---

### ν• μ¤„ μ •λ¦¬

> vLLMμ€ OpenAI νΈν™ RESTλ΅ μ¶©λ¶„
> 
> 
> **gRPCλ” Tritonμ²λΌ λ€μ©λ‰ ν…μ„ μ „μ†΅ν•  λ• μλ―Έ μμ**
>